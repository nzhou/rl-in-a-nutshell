{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the FUN track of WiDS reinforement learning in a nutshell tutorial!\n",
    "\n",
    "In this track, you will play with several RL libraries that provide\n",
    "- Standard environments to train and compare different algorithms\n",
    "- Easy-to-use pre-implemented algorithms\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Windows, please first install msmpi form https://www.microsoft.com/en-us/download/details.aspx?id=57467,\n",
    "# and then follow the instructions at https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30\n",
    "!conda update conda\n",
    "!conda update conda-build\n",
    "!conda install swig\n",
    "\n",
    "# For Linux and MacOS, you can run this cell directly.\n",
    "!apt install swig # Remember to uncomment this line if you are running in Windows.\n",
    "!pip install --upgrade pip\n",
    "!pip install gym==0.12.1\n",
    "!pip install stable_baselines==2.4.1\n",
    "!pip install box2d==2.3.2\n",
    "!pip install box2d-kengz\n",
    "!pip install tensorforce==0.4.3",
    "\n",
    "# If you run into issues installing box2d, please try to build it from source by uncommenting the following lines.\n",
    "#!sudo apt-get install --reinstall build-essential\n",
    "#!pip install git+https://github.com/pybox2d/pybox2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the simulation of the model in the environment\n",
    "# For vectorized models like PPO and A2C, set use_vec_env to True\n",
    "def render(env_id, use_vec_env=False, model=None, max_step=500):\n",
    "    env = gym.make(env_id)\n",
    "    if use_vec_env:\n",
    "        # Note: Vectorized environments allow multiprocess training. \n",
    "        # In this tutorial, we only uses one process, so we use the DummyVecEnv which is just a simple wrapper.\n",
    "\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        env.render()\n",
    "        if (model==None): # Sample a random action from the action space if no model is provided\n",
    "            if use_vec_env:\n",
    "                action = [env.action_space.sample()]\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _states = model.predict(observation)\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "\n",
    "    if use_vec_env:\n",
    "        env.envs[0].close()\n",
    "    else:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the MlpPolicy because input of CartPole is a feature vector, not images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "# Training parameters\n",
    "policy = \"MlpPolicy\"\n",
    "max_train_step = 100000\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model_path = \"./models/CartPole_PPO.model\"\n",
    "log_path = \"./log/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "render(env_id, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier, we use a variation of the original OpenAI baselines: [stable baselines](https://github.com/hill-a/stable-baselines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(policy, env_id, learning_rate=learning_rate, tensorboard_log=log_path)\n",
    "model.learn(max_train_step, tb_log_name=env_id+str(time.time()))\n",
    "# Save the agent\n",
    "model.save(model_path)\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(policy, env_id).load(model_path)\n",
    "render(env_id, use_vec_env=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(policy, env_id, learning_rate=learning_rate, tensorboard_log=log_path)\n",
    "model.learn(max_train_step, tb_log_name=env_id+str(time.time()))\n",
    "# Save the agent\n",
    "model.save(model_path)\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(policy, env_id).load(model_path)\n",
    "render(env_id, use_vec_env=True, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another environment with another model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# Training parameters\n",
    "policy = \"MlpPolicy\"\n",
    "max_train_step = 100000\n",
    "\n",
    "model_path = \"./models/LunarLander_A2C.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy, env_id, ent_coef=0.1, learning_rate=learning_rate, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=max_train_step, tb_log_name=env_id+str(time.time()))\n",
    "# Save the agent\n",
    "model.save(model_path)\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = A2C(policy, env_id, ent_coef=0.1).load(model_path)\n",
    "\n",
    "# Enjoy trained agent\n",
    "render(env_id, use_vec_env=True, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorforce\n",
    "\n",
    "[Tensorforce](https://github.com/tensorforce/tensorforce) is an open-source library that provides modulized APIs for reinforcement learning. As the name suggest, it is built on top of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAIgym environment\n",
    "env = OpenAIGym('CartPole-v1', visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='dense', size=32, activation='tanh'),\n",
    "    dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states=env.states,\n",
    "    actions=env.actions,\n",
    "    network=network_spec,\n",
    "    batching_capacity=4096,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    entropy_regularization=0.01,\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "#    summarizer=dict(directory=\"./board/\",\n",
    "#                    steps=50,\n",
    "#                    labels=['graph',\n",
    "#                            'configuration',\n",
    "#                            'gradients_scalar',\n",
    "#                            'regularization',\n",
    "#                            'inputs',\n",
    "#                            'losses'\n",
    "#                            'variables'\n",
    "#                           ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=100, max_episode_timesteps=200, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.agent.save_model(directory=\"./agents/\")\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
